{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1yS1qck4fUie"
      },
      "source": [
        "# **GPU check**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ovX5D8Y9AXZh"
      },
      "outputs": [],
      "source": [
        "gpu_info = !nvidia-smi\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "if gpu_info.find('failed') >= 0:\n",
        "  print('Not connected to a GPU')\n",
        "else:\n",
        "  print(gpu_info)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hZX4FgBufbml"
      },
      "source": [
        "# **Pendulumv1 Implementation**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UA6qnj8CcLKy",
        "outputId": "9600ee7f-3a4d-4f25-fe41-11fea6d7f116"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting stable-baselines3[extra]\n",
            "  Downloading stable_baselines3-2.4.0-py3-none-any.whl.metadata (4.5 kB)\n",
            "Collecting gymnasium<1.1.0,>=0.29.1 (from stable-baselines3[extra])\n",
            "  Using cached gymnasium-1.0.0-py3-none-any.whl.metadata (9.5 kB)\n",
            "Requirement already satisfied: numpy<2.0,>=1.20 in /usr/local/lib/python3.10/dist-packages (from stable-baselines3[extra]) (1.26.4)\n",
            "Requirement already satisfied: torch>=1.13 in /usr/local/lib/python3.10/dist-packages (from stable-baselines3[extra]) (2.5.1+cu121)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.10/dist-packages (from stable-baselines3[extra]) (3.1.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from stable-baselines3[extra]) (2.2.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from stable-baselines3[extra]) (3.8.0)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.10/dist-packages (from stable-baselines3[extra]) (4.10.0.84)\n",
            "Requirement already satisfied: pygame in /usr/local/lib/python3.10/dist-packages (from stable-baselines3[extra]) (2.6.1)\n",
            "Requirement already satisfied: tensorboard>=2.9.1 in /usr/local/lib/python3.10/dist-packages (from stable-baselines3[extra]) (2.17.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from stable-baselines3[extra]) (5.9.5)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from stable-baselines3[extra]) (4.66.6)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from stable-baselines3[extra]) (13.9.4)\n",
            "Collecting ale-py>=0.9.0 (from stable-baselines3[extra])\n",
            "  Downloading ale_py-0.10.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.6 kB)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.10/dist-packages (from stable-baselines3[extra]) (11.0.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from ale-py>=0.9.0->stable-baselines3[extra]) (4.12.2)\n",
            "Collecting farama-notifications>=0.0.1 (from gymnasium<1.1.0,>=0.29.1->stable-baselines3[extra])\n",
            "  Using cached Farama_Notifications-0.0.4-py3-none-any.whl.metadata (558 bytes)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (1.4.0)\n",
            "Requirement already satisfied: grpcio>=1.48.2 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (1.68.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (3.7)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (24.2)\n",
            "Requirement already satisfied: protobuf!=4.24.0,>=3.19.6 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (4.25.5)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (75.1.0)\n",
            "Requirement already satisfied: six>1.9 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (1.16.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (3.1.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable-baselines3[extra]) (3.16.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable-baselines3[extra]) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable-baselines3[extra]) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable-baselines3[extra]) (2024.10.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable-baselines3[extra]) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=1.13->stable-baselines3[extra]) (1.3.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable-baselines3[extra]) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable-baselines3[extra]) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable-baselines3[extra]) (4.55.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable-baselines3[extra]) (1.4.7)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable-baselines3[extra]) (3.2.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable-baselines3[extra]) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->stable-baselines3[extra]) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->stable-baselines3[extra]) (2024.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->stable-baselines3[extra]) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->stable-baselines3[extra]) (2.18.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->stable-baselines3[extra]) (0.1.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard>=2.9.1->stable-baselines3[extra]) (3.0.2)\n",
            "Downloading ale_py-0.10.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m23.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hUsing cached gymnasium-1.0.0-py3-none-any.whl (958 kB)\n",
            "Downloading stable_baselines3-2.4.0-py3-none-any.whl (183 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m183.9/183.9 kB\u001b[0m \u001b[31m16.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hUsing cached Farama_Notifications-0.0.4-py3-none-any.whl (2.5 kB)\n",
            "Installing collected packages: farama-notifications, gymnasium, ale-py, stable-baselines3\n",
            "Successfully installed ale-py-0.10.1 farama-notifications-0.0.4 gymnasium-1.0.0 stable-baselines3-2.4.0\n",
            "Requirement already satisfied: gymnasium in /usr/local/lib/python3.10/dist-packages (1.0.0)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (1.26.4)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (3.1.0)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (4.12.2)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (0.0.4)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (3.8.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (4.55.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.4.7)\n",
            "Requirement already satisfied: numpy<2,>=1.21 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (24.2)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (11.0.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (3.2.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n"
          ]
        }
      ],
      "source": [
        "# Install required libraries\n",
        "!pip install stable-baselines3[extra]\n",
        "!pip install gymnasium\n",
        "!pip install matplotlib\n",
        "\n",
        "# Import necessary libraries\n",
        "import os\n",
        "import gymnasium as gym\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import math\n",
        "import torch  # Needed for tensor operations in visualization\n",
        "from stable_baselines3 import SAC\n",
        "from stable_baselines3.common.callbacks import EvalCallback, BaseCallback\n",
        "from stable_baselines3.common.monitor import Monitor\n",
        "from stable_baselines3.common.utils import get_linear_fn\n",
        "from stable_baselines3.common.evaluation import evaluate_policy\n",
        "from gymnasium.wrappers import TimeLimit"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7PijXk7FYOx_"
      },
      "outputs": [],
      "source": [
        "# ====== 1. Set Up the Environment ======\n",
        "# Specify the log directory\n",
        "log_dir = \"./monitor_logs/\"\n",
        "os.makedirs(log_dir, exist_ok=True)\n",
        "\n",
        "# Choose the environment: 'Pendulum-v1' or 'LunarLanderContinuous-v2'\n",
        "env_name = \"Pendulum-v1\"  # Change to \"LunarLanderContinuous-v2\" for a more complex task\n",
        "\n",
        "# Wrap the environment with Monitor and TimeLimit wrappers\n",
        "env = gym.make(env_name)\n",
        "if env_name == \"Pendulum-v1\":\n",
        "    env = TimeLimit(env, max_episode_steps=200)  # Limit the episode length\n",
        "env = Monitor(env, log_dir + \"monitor.csv\", allow_early_resets=True)\n",
        "\n",
        "# Create a separate evaluation environment\n",
        "eval_env = gym.make(env_name)\n",
        "if env_name == \"Pendulum-v1\":\n",
        "    eval_env = TimeLimit(eval_env, max_episode_steps=200)\n",
        "eval_env = Monitor(eval_env)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "j5f2_HNMYVTI"
      },
      "outputs": [],
      "source": [
        "# ====== 2. Define TrainCallback to Log Losses and Entropy ======\n",
        "class TrainCallback(BaseCallback):\n",
        "    def __init__(self, verbose=0):\n",
        "        super(TrainCallback, self).__init__(verbose)\n",
        "        self.policy_losses = []\n",
        "        self.q_losses = []\n",
        "        self.entropy_coefficients = []\n",
        "\n",
        "    def _on_step(self) -> bool:\n",
        "        logs = self.logger.name_to_value\n",
        "        if \"train/actor_loss\" in logs:\n",
        "            self.policy_losses.append(logs[\"train/actor_loss\"])\n",
        "        if \"train/critic_loss\" in logs:\n",
        "            self.q_losses.append(logs[\"train/critic_loss\"])\n",
        "        if \"train/ent_coef\" in logs:\n",
        "            self.entropy_coefficients.append(logs[\"train/ent_coef\"])\n",
        "\n",
        "        return True\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ====== 3. Define Custom Learning Rate Schedule (Cosine Annealing) ======\n",
        "def cosine_annealing(initial_lr, min_lr):\n",
        "    def lr_schedule(progress_remaining):\n",
        "        return min_lr + (initial_lr - min_lr) * 0.5 * (1 + math.cos(math.pi * (1 - progress_remaining)))\n",
        "    return lr_schedule\n",
        "\n",
        "learning_rate_schedule = cosine_annealing(initial_lr=0.001, min_lr=0.0001)"
      ],
      "metadata": {
        "id": "Vg4kCmCUMvU8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_ktlpqG4YesI"
      },
      "outputs": [],
      "source": [
        "# ====== 4. Initialize SAC Model ======\n",
        "model = SAC(\n",
        "    \"MlpPolicy\",\n",
        "    env,\n",
        "    verbose=1,\n",
        "    learning_rate=learning_rate_schedule,  # Dynamic LR schedule\n",
        "    ent_coef=\"auto\",\n",
        "    tensorboard_log=\"./sac_tensorboard/\"\n",
        ")\n",
        "\n",
        "# Define evaluation callback\n",
        "eval_callback = EvalCallback(eval_env, best_model_save_path=\"./logs/\",\n",
        "                             log_path=\"./logs/\", eval_freq=1000)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "t8DXC5a6YwGE",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 207
        },
        "outputId": "521bec28-c046-419b-936b-9d4048469b2b"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'eval_callback' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-b6e57926a5b1>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# ====== 5. Train the SAC Agent ======\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mtrain_callback\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTrainCallback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtotal_timesteps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0meval_callback\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_callback\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# ====== 6. Plot Learning Curve ======\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'eval_callback' is not defined"
          ]
        }
      ],
      "source": [
        "# ====== 5. Train the SAC Agent ======\n",
        "train_callback = TrainCallback()\n",
        "model.learn(total_timesteps=100000, callback=[eval_callback, train_callback])\n",
        "\n",
        "# ====== 6. Plot Learning Curve ======\n",
        "# Check if the monitor log file was created\n",
        "monitor_file = log_dir + \"monitor.csv\"\n",
        "if os.path.exists(monitor_file):\n",
        "    log_data = pd.read_csv(monitor_file, skiprows=1)\n",
        "    # Plot the learning curve\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.plot(log_data['l'].cumsum(), log_data['r'], label='Reward per Episode')\n",
        "    plt.xlabel('Timesteps')\n",
        "    plt.ylabel('Reward')\n",
        "    plt.title('Learning Curve (SAC)')\n",
        "    plt.legend()\n",
        "    plt.grid()\n",
        "    plt.show()\n",
        "else:\n",
        "    print(\"Monitor log file not found. Ensure the environment is wrapped correctly.\")\n",
        "\n",
        "# ====== 7. Plot Policy Loss, Q-Loss, and Entropy ======\n",
        "# Plot Policy Loss\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(train_callback.policy_losses, label='Policy Loss', color='blue')\n",
        "plt.xlabel('Training Steps')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Policy Loss During Training')\n",
        "plt.legend()\n",
        "plt.grid()\n",
        "plt.show()\n",
        "\n",
        "# Plot Q-Loss\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(train_callback.q_losses, label='Q Loss', color='orange')\n",
        "plt.xlabel('Training Steps')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Q-Loss During Training')\n",
        "plt.legend()\n",
        "plt.grid()\n",
        "plt.show()\n",
        "\n",
        "# Plot Entropy Coefficient\n",
        "if train_callback.entropy_coefficients:\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.plot(train_callback.entropy_coefficients, label='Entropy Coefficient', color='green')\n",
        "    plt.xlabel('Training Steps')\n",
        "    plt.ylabel('Entropy Coefficient')\n",
        "    plt.title('Entropy Coefficient During Training')\n",
        "    plt.legend()\n",
        "    plt.grid()\n",
        "    plt.show()\n",
        "else:\n",
        "    print(\"No entropy coefficient data found. Ensure logging is set up correctly.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "apUtQN6VYw0F"
      },
      "outputs": [],
      "source": [
        "# ====== 8. Visualize Action Distribution ======\n",
        "def visualize_action_distribution(model, env, n_samples=1000):\n",
        "    \"\"\"\n",
        "    Visualizes the distribution of actions taken by the policy.\n",
        "    \"\"\"\n",
        "    actions = []\n",
        "    obs, _ = env.reset()\n",
        "    for _ in range(n_samples):\n",
        "        action, _ = model.predict(obs, deterministic=False)\n",
        "        actions.append(action[0])  # Assuming 1D action space\n",
        "        obs, _, terminated, truncated, _ = env.step(action)\n",
        "        if terminated or truncated:\n",
        "            obs, _ = env.reset()\n",
        "\n",
        "    actions = np.array(actions)\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.hist(actions, bins=50, density=True, label='Action Distribution')\n",
        "    plt.xlabel('Action Value')\n",
        "    plt.ylabel('Density')\n",
        "    plt.title('Policy Action Distribution')\n",
        "    plt.legend()\n",
        "    plt.grid()\n",
        "    plt.show()\n",
        "\n",
        "# Call the function to visualize action distribution\n",
        "visualize_action_distribution(model, env)\n",
        "\n",
        "# ====== 9. Visualize Q-Value Heatmap ======\n",
        "def plot_q_values(model, n_points=100):\n",
        "    \"\"\"\n",
        "    Plots a heatmap of Q-values over a range of states and actions.\n",
        "    \"\"\"\n",
        "    # For Pendulum-v1, the state consists of [cos(theta), sin(theta), theta_dot]\n",
        "    # We'll fix theta_dot to 0 for simplicity and vary theta from -pi to pi\n",
        "    theta = np.linspace(-np.pi, np.pi, n_points)\n",
        "    theta_dot = 0.0\n",
        "    actions = np.linspace(-2.0, 2.0, n_points)  # Action range for Pendulum-v1\n",
        "    q_values = np.zeros((n_points, n_points))\n",
        "\n",
        "    for i, t in enumerate(theta):\n",
        "        for j, a in enumerate(actions):\n",
        "            obs = np.array([np.cos(t), np.sin(t), theta_dot])\n",
        "            action = np.array([a])\n",
        "            obs_tensor = torch.tensor(obs.reshape(1, -1), dtype=torch.float32).to(model.device)\n",
        "            action_tensor = torch.tensor(action.reshape(1, -1), dtype=torch.float32).to(model.device)\n",
        "            with torch.no_grad():\n",
        "                q_value = model.critic_target(obs_tensor, action_tensor)[0].cpu().numpy()\n",
        "            q_values[i, j] = q_value\n",
        "\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.imshow(q_values, extent=[actions.min(), actions.max(), theta.min(), theta.max()], origin='lower', aspect='auto', cmap='viridis')\n",
        "    plt.colorbar(label='Q-value')\n",
        "    plt.xlabel('Action')\n",
        "    plt.ylabel('Theta')\n",
        "    plt.title('Q-value Heatmap')\n",
        "    plt.show()\n",
        "\n",
        "# Call the function to plot Q-value heatmap\n",
        "plot_q_values(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gi8MVW9bY4uP"
      },
      "outputs": [],
      "source": [
        "# ====== 10. Evaluate the Agent ======\n",
        "mean_reward, std_reward = evaluate_policy(model, eval_env, n_eval_episodes=10)\n",
        "print(f\"Mean reward: {mean_reward:.2f} ± {std_reward:.2f}\")\n",
        "\n",
        "# ====== 11. Visualize the Trained Agent ======\n",
        "obs, _ = env.reset()\n",
        "for _ in range(1000):\n",
        "    action, _states = model.predict(obs, deterministic=True)\n",
        "    obs, reward, terminated, truncated, info = env.step(action)\n",
        "    env.render()\n",
        "    if terminated or truncated:\n",
        "        obs, _ = env.reset()\n",
        "env.close()\n",
        "\n",
        "# ====== Optional: TensorBoard ======\n",
        "%load_ext tensorboard\n",
        "%tensorboard --logdir ./sac_tensorboard/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ouQcSaDaZFfS"
      },
      "outputs": [],
      "source": [
        "# ====== 12. Theoretical Explanation (Add as comments or markdown in your notebook) ======\n",
        "\n",
        "\"\"\"\n",
        "## Theoretical Explanation\n",
        "\n",
        "### Soft Actor-Critic (SAC)\n",
        "SAC is an off-policy actor-critic algorithm that aims to maximize both the expected return and the entropy of the policy. The entropy term encourages exploration by penalizing certainty in the policy's action selection.\n",
        "\n",
        "### Entropy Maximization\n",
        "The policy is trained to maximize the expected reward while also maximizing entropy. This is achieved by adding an entropy term to the reward:\n",
        "\\[ J(\\pi) = \\sum_{t} \\mathbb{E}_{(s_t, a_t) \\sim \\rho_{\\pi}} [r(s_t, a_t) + \\alpha \\mathcal{H}(\\pi(\\cdot|s_t))] \\]\n",
        "where \\( \\alpha \\) is the entropy coefficient that balances the trade-off.\n",
        "\n",
        "### Cosine Annealing Learning Rate Schedule\n",
        "A cosine annealing schedule gradually decreases the learning rate over time following a cosine curve. This helps in making large updates initially for rapid learning and smaller updates later for fine-tuning.\n",
        "\n",
        "### Visualization of Action Distribution\n",
        "The action distribution plot shows how the policy outputs actions over time. A concentrated distribution indicates deterministic behavior, while a spread distribution indicates exploration.\n",
        "\n",
        "### Q-value Heatmap\n",
        "The Q-value heatmap visualizes the expected return (Q-value) for different state-action pairs. It helps in understanding how the agent evaluates the utility of actions in different states.\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "# ====== 13. Optional: Apply to a More Complex Environment ======\n",
        "# To apply the same code to 'LunarLanderContinuous-v2', update the environment:\n",
        "\n",
        "\"\"\"\n",
        "# Uncomment the following lines to switch to LunarLanderContinuous-v2\n",
        "\n",
        "env_name = \"LunarLanderContinuous-v2\"\n",
        "\n",
        "env = gym.make(env_name)\n",
        "env = Monitor(env, log_dir + \"monitor.csv\", allow_early_resets=True)\n",
        "eval_env = gym.make(env_name)\n",
        "eval_env = Monitor(eval_env)\n",
        "\n",
        "# Ensure to adjust the action ranges and visualization functions accordingly.\n",
        "# For example, the action space in LunarLanderContinuous-v2 is 2D.\n",
        "\"\"\"\n",
        "\n",
        "# Note: Remember to adjust the action dimensions in the visualization functions for a new environment."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ArDToEoEfOZI"
      },
      "source": [
        "# **LunarLanderContinuous Implementation**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "TpstjNTpdxb1",
        "outputId": "2fe1f44f-a684-4f74-f34f-26c50c6e2446"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting optuna\n",
            "  Downloading optuna-4.1.0-py3-none-any.whl.metadata (16 kB)\n",
            "Collecting alembic>=1.5.0 (from optuna)\n",
            "  Downloading alembic-1.14.0-py3-none-any.whl.metadata (7.4 kB)\n",
            "Collecting colorlog (from optuna)\n",
            "  Downloading colorlog-6.9.0-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from optuna) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from optuna) (24.2)\n",
            "Requirement already satisfied: sqlalchemy>=1.4.2 in /usr/local/lib/python3.10/dist-packages (from optuna) (2.0.36)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from optuna) (4.66.6)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from optuna) (6.0.2)\n",
            "Collecting Mako (from alembic>=1.5.0->optuna)\n",
            "  Downloading Mako-1.3.8-py3-none-any.whl.metadata (2.9 kB)\n",
            "Requirement already satisfied: typing-extensions>=4 in /usr/local/lib/python3.10/dist-packages (from alembic>=1.5.0->optuna) (4.12.2)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from sqlalchemy>=1.4.2->optuna) (3.1.1)\n",
            "Requirement already satisfied: MarkupSafe>=0.9.2 in /usr/local/lib/python3.10/dist-packages (from Mako->alembic>=1.5.0->optuna) (3.0.2)\n",
            "Downloading optuna-4.1.0-py3-none-any.whl (364 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m364.4/364.4 kB\u001b[0m \u001b[31m26.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading alembic-1.14.0-py3-none-any.whl (233 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m233.5/233.5 kB\u001b[0m \u001b[31m19.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading colorlog-6.9.0-py3-none-any.whl (11 kB)\n",
            "Downloading Mako-1.3.8-py3-none-any.whl (78 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.6/78.6 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: Operation cancelled by user\u001b[0m\u001b[31m\n",
            "\u001b[0mCollecting swig\n",
            "  Downloading swig-4.3.0-py2.py3-none-manylinux_2_5_x86_64.manylinux1_x86_64.whl.metadata (3.5 kB)\n",
            "Downloading swig-4.3.0-py2.py3-none-manylinux_2_5_x86_64.manylinux1_x86_64.whl (1.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m65.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hTraceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pkg_resources/__init__.py\", line 3070, in _dep_map\n",
            "    return self.__dep_map\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pkg_resources/__init__.py\", line 2863, in __getattr__\n",
            "    raise AttributeError(attr)\n",
            "AttributeError: _DistInfoDistribution__dep_map\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/cli/base_command.py\", line 179, in exc_logging_wrapper\n",
            "    status = run_func(*args)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/cli/req_command.py\", line 67, in wrapper\n",
            "    return func(self, options, args)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/commands/install.py\", line 447, in run\n",
            "    conflicts = self._determine_conflicts(to_install)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/commands/install.py\", line 578, in _determine_conflicts\n",
            "    return check_install_conflicts(to_install)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/operations/check.py\", line 101, in check_install_conflicts\n",
            "    package_set, _ = create_package_set_from_installed()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/operations/check.py\", line 42, in create_package_set_from_installed\n",
            "    dependencies = list(dist.iter_dependencies())\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/metadata/pkg_resources.py\", line 247, in iter_dependencies\n",
            "    return self._dist.requires(extras)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pkg_resources/__init__.py\", line 2786, in requires\n",
            "    dm = self._dep_map\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pkg_resources/__init__.py\", line 3072, in _dep_map\n",
            "    self.__dep_map = self._compute_dependencies()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pkg_resources/__init__.py\", line 3082, in _compute_dependencies\n",
            "    reqs.extend(parse_requirements(req))\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pkg_resources/__init__.py\", line 3135, in __init__\n",
            "    super().__init__(requirement_string)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/packaging/requirements.py\", line 36, in __init__\n",
            "    parsed = _parse_requirement(requirement_string)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/packaging/_parser.py\", line 62, in parse_requirement\n",
            "    return _parse_requirement(Tokenizer(source, rules=DEFAULT_RULES))\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/packaging/_parser.py\", line 71, in _parse_requirement\n",
            "    name_token = tokenizer.expect(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/packaging/_tokenizer.py\", line 136, in expect\n",
            "    def expect(self, name: str, *, expected: str) -> Token:\n",
            "KeyboardInterrupt\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/bin/pip3\", line 8, in <module>\n",
            "    sys.exit(main())\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/cli/main.py\", line 80, in main\n",
            "    return command.main(cmd_args)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/cli/base_command.py\", line 100, in main\n",
            "    return self._main(args)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/cli/base_command.py\", line 232, in _main\n",
            "    return run(options, args)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/cli/base_command.py\", line 215, in exc_logging_wrapper\n",
            "    logger.critical(\"Operation cancelled by user\")\n",
            "  File \"/usr/lib/python3.10/logging/__init__.py\", line 1524, in critical\n",
            "    self._log(CRITICAL, msg, args, **kwargs)\n",
            "  File \"/usr/lib/python3.10/logging/__init__.py\", line 1612, in _log\n",
            "    fn, lno, func, sinfo = self.findCaller(stack_info, stacklevel)\n",
            "  File \"/usr/lib/python3.10/logging/__init__.py\", line 1560, in findCaller\n",
            "    while f and stacklevel > 1:\n",
            "KeyboardInterrupt\n",
            "^C\n",
            "Collecting gymnasium[box2d]\n",
            "  Downloading gymnasium-1.0.0-py3-none-any.whl.metadata (9.5 kB)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium[box2d]) (1.26.4)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium[box2d]) (3.1.0)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium[box2d]) (4.12.2)\n",
            "Collecting farama-notifications>=0.0.1 (from gymnasium[box2d])\n",
            "  Downloading Farama_Notifications-0.0.4-py3-none-any.whl.metadata (558 bytes)\n",
            "Collecting box2d-py==2.3.5 (from gymnasium[box2d])\n",
            "  Downloading box2d-py-2.3.5.tar.gz (374 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m374.4/374.4 kB\u001b[0m \u001b[31m23.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: pygame>=2.1.3 in /usr/local/lib/python3.10/dist-packages (from gymnasium[box2d]) (2.6.1)\n",
            "Collecting swig==4.* (from gymnasium[box2d])\n",
            "  Using cached swig-4.3.0-py2.py3-none-manylinux_2_5_x86_64.manylinux1_x86_64.whl.metadata (3.5 kB)\n",
            "Using cached swig-4.3.0-py2.py3-none-manylinux_2_5_x86_64.manylinux1_x86_64.whl (1.9 MB)\n",
            "Downloading Farama_Notifications-0.0.4-py3-none-any.whl (2.5 kB)\n",
            "Downloading gymnasium-1.0.0-py3-none-any.whl (958 kB)\n",
            "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m952.3/958.1 kB\u001b[0m \u001b[31m145.9 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: Operation cancelled by user\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        },
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'gymnasium'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-5a22e5254c13>\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mgymnasium\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mgym\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'gymnasium'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ],
      "source": [
        "!pip install optuna\n",
        "!pip install swig\n",
        "!pip install \"gymnasium[box2d]\"\n",
        "\n",
        "import os\n",
        "import gymnasium as gym\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import math\n",
        "import torch\n",
        "import optuna\n",
        "from stable_baselines3 import SAC\n",
        "from stable_baselines3.common.callbacks import EvalCallback\n",
        "from stable_baselines3.common.monitor import Monitor\n",
        "from stable_baselines3.common.evaluation import evaluate_policy\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_Z9baQS5ga0L"
      },
      "outputs": [],
      "source": [
        "# Define cosine annealing schedule\n",
        "def cosine_annealing(initial_lr, min_lr):\n",
        "    def lr_schedule(progress_remaining):\n",
        "        return min_lr + (initial_lr - min_lr) * 0.5 * (1 + math.cos(math.pi * (1 - progress_remaining)))\n",
        "    return lr_schedule\n",
        "\n",
        "env_name = \"LunarLanderContinuous-v3\"\n",
        "log_dir = \"./monitor_logs/\"\n",
        "os.makedirs(log_dir, exist_ok=True)\n",
        "\n",
        "env = gym.make(env_name)\n",
        "env = Monitor(env, log_dir + \"monitor.csv\", allow_early_resets=True)\n",
        "eval_env = gym.make(env_name, render_mode=\"rgb_array\")\n",
        "eval_env = Monitor(eval_env)\n",
        "\n",
        "from stable_baselines3.common.callbacks import BaseCallback\n",
        "\n",
        "class TrainCallback(BaseCallback):\n",
        "    def __init__(self, verbose=0):\n",
        "        super(TrainCallback, self).__init__(verbose)\n",
        "        self.policy_losses = []\n",
        "        self.q_losses = []\n",
        "        self.entropy_coefficients = []\n",
        "\n",
        "    def _on_step(self) -> bool:\n",
        "        logs = self.logger.name_to_value\n",
        "        if \"train/actor_loss\" in logs:\n",
        "            self.policy_losses.append(logs[\"train/actor_loss\"])\n",
        "        if \"train/critic_loss\" in logs:\n",
        "            self.q_losses.append(logs[\"train/critic_loss\"])\n",
        "        if \"train/ent_coef\" in logs:\n",
        "            self.entropy_coefficients.append(logs[\"train/ent_coef\"])\n",
        "        return True\n",
        "\n",
        "def optimize_sac(trial):\n",
        "    initial_lr = trial.suggest_float(\"initial_lr\", 1e-4, 1e-3, log=True)\n",
        "    min_lr = trial.suggest_float(\"min_lr\", 1e-5, 5e-4, log=True)\n",
        "    batch_size = trial.suggest_categorical(\"batch_size\", [256, 512, 1024])\n",
        "    target_entropy = trial.suggest_float(\"target_entropy\", -2.0, -0.5)\n",
        "\n",
        "    learning_rate_schedule = cosine_annealing(initial_lr=initial_lr, min_lr=min_lr)\n",
        "\n",
        "    train_callback = TrainCallback()\n",
        "\n",
        "    model = SAC(\n",
        "        \"MlpPolicy\",\n",
        "        env,\n",
        "        learning_rate=learning_rate_schedule,\n",
        "        batch_size=batch_size,\n",
        "        ent_coef=\"auto\",\n",
        "        target_entropy=target_entropy,\n",
        "        verbose=0,\n",
        "        device=\"cuda\"\n",
        "    )\n",
        "\n",
        "    model.learn(total_timesteps=20000, callback=train_callback)\n",
        "\n",
        "    mean_reward, _ = evaluate_policy(model, eval_env, n_eval_episodes=10)\n",
        "    return mean_reward\n",
        "\n",
        "# Run Optuna hyperparameter tuning\n",
        "study = optuna.create_study(direction=\"maximize\")\n",
        "study.optimize(optimize_sac, n_trials=10)\n",
        "\n",
        "# Print best parameters\n",
        "print(\"Best Hyperparameters:\", study.best_params)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Use best hyperparameters for full training\n",
        "best_params = study.best_params\n",
        "learning_rate_schedule = cosine_annealing(\n",
        "    initial_lr=best_params[\"initial_lr\"],\n",
        "    min_lr=best_params[\"min_lr\"]\n",
        ")\n",
        "\n",
        "model = SAC(\n",
        "    \"MlpPolicy\",\n",
        "    env,\n",
        "    learning_rate=learning_rate_schedule,\n",
        "    batch_size=best_params[\"batch_size\"],\n",
        "    ent_coef=\"auto\",\n",
        "    target_entropy=best_params[\"target_entropy\"],\n",
        "    verbose=1,\n",
        "    device=\"cuda\",\n",
        "    tensorboard_log=\"./sac_tensorboard/\"\n",
        ")\n",
        "\n",
        "eval_callback = EvalCallback(eval_env, best_model_save_path=\"./logs/\",\n",
        "                             log_path=\"./logs/\", eval_freq=1000)\n",
        "\n",
        "train_callback = TrainCallback()\n",
        "model.learn(total_timesteps=100000, callback=[eval_callback, train_callback])\n"
      ],
      "metadata": {
        "id": "GenPdiTSO9L2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from gymnasium.wrappers import RecordVideo\n",
        "\n",
        "# Specify directory to save the video\n",
        "video_dir = \"./videos/\"\n",
        "os.makedirs(video_dir, exist_ok=True)\n",
        "\n",
        "# Wrap the environment for video recording\n",
        "video_env = RecordVideo(eval_env, video_folder=video_dir, episode_trigger=lambda episode_id: True)\n",
        "\n",
        "# Run the trained agent and record the video\n",
        "obs, _ = video_env.reset()\n",
        "for _ in range(1000):  # Adjust the number of steps if needed\n",
        "    action, _states = model.predict(obs, deterministic=True)  # Use deterministic actions for video\n",
        "    obs, reward, terminated, truncated, _ = video_env.step(action)\n",
        "    if terminated or truncated:\n",
        "        break\n",
        "\n",
        "video_env.close()\n",
        "\n",
        "from IPython.display import Video\n",
        "\n",
        "# Find the latest video file in the directory\n",
        "video_path = sorted([os.path.join(video_dir, f) for f in os.listdir(video_dir) if f.endswith(\".mp4\")])[-1]\n",
        "Video(video_path, embed=True, width=600, height=400)\n",
        "\n",
        "from google.colab import files\n",
        "\n",
        "# Download the video\n",
        "files.download(video_path)\n"
      ],
      "metadata": {
        "id": "CT7Xy7GcP4d9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load training log from monitor\n",
        "monitor_file = log_dir + \"monitor.csv\"\n",
        "if os.path.exists(monitor_file):\n",
        "    log_data = pd.read_csv(monitor_file, skiprows=1)\n",
        "    # Plot the cumulative reward\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.plot(log_data['l'].cumsum(), log_data['r'], label='Episode Reward')\n",
        "    plt.xlabel('Timesteps')\n",
        "    plt.ylabel('Reward')\n",
        "    plt.title('Learning Curve')\n",
        "    plt.legend()\n",
        "    plt.grid()\n",
        "    plt.show()\n",
        "else:\n",
        "    print(\"Monitor log file not found. Ensure the environment is wrapped correctly.\")\n"
      ],
      "metadata": {
        "id": "hrevY1RONSc4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Assuming you have logged the losses during training in TrainCallback\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(train_callback.policy_losses, label='Policy Loss', color='blue')\n",
        "plt.xlabel('Training Steps')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Policy Loss Over Training')\n",
        "plt.legend()\n",
        "plt.grid()\n",
        "plt.show()\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(train_callback.q_losses, label='Q Loss', color='orange')\n",
        "plt.xlabel('Training Steps')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Q-Loss Over Training')\n",
        "plt.legend()\n",
        "plt.grid()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "ocPW2ql2NUQW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the policy and get rewards per episode\n",
        "episode_rewards, _ = evaluate_policy(model, eval_env, n_eval_episodes=20, return_episode_rewards=True)\n",
        "\n",
        "# Convert list of episode rewards to numpy array for easier manipulation\n",
        "episode_rewards = np.array(episode_rewards)\n",
        "\n",
        "# Plot episode rewards\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(range(len(episode_rewards)), episode_rewards, label='Episode Reward')\n",
        "plt.xlabel('Evaluation Episodes')\n",
        "plt.ylabel('Reward')\n",
        "plt.title('Agent Performance Over Evaluation Episodes')\n",
        "plt.legend()\n",
        "plt.grid()\n",
        "plt.show()\n",
        "\n",
        "# Print summary statistics\n",
        "mean_reward = np.mean(episode_rewards)\n",
        "std_reward = np.std(episode_rewards)\n",
        "print(f\"Mean reward: {mean_reward:.2f} ± {std_reward:.2f}\")\n"
      ],
      "metadata": {
        "id": "XeoZAPmGNc3e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def visualize_action_distribution(model, env, n_samples=1000):\n",
        "    actions = []\n",
        "    obs, _ = env.reset()\n",
        "    for _ in range(n_samples):\n",
        "        action, _ = model.predict(obs, deterministic=False)\n",
        "        actions.append(action)\n",
        "        obs, _, terminated, truncated, _ = env.step(action)\n",
        "        if terminated or truncated:\n",
        "            obs, _ = env.reset()\n",
        "\n",
        "    actions = np.array(actions)\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    for i in range(actions.shape[1]):  # Loop over action dimensions\n",
        "        plt.hist(actions[:, i], bins=50, alpha=0.5, label=f'Action Dimension {i+1}')\n",
        "    plt.xlabel('Action Value')\n",
        "    plt.ylabel('Frequency')\n",
        "    plt.title('Action Distribution')\n",
        "    plt.legend()\n",
        "    plt.grid()\n",
        "    plt.show()\n",
        "\n",
        "# Call the function to plot action distribution\n",
        "visualize_action_distribution(model, eval_env)\n"
      ],
      "metadata": {
        "id": "ec3ZaVv_NxyF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_q_values(model, fixed_state=None, n_points=100):\n",
        "    \"\"\"\n",
        "    Plots a heatmap of Q-values for a fixed state and varying actions in LunarLanderContinuous-v3.\n",
        "    \"\"\"\n",
        "    # Define action range\n",
        "    actions = np.linspace(-1.0, 1.0, n_points)  # Action range for LunarLanderContinuous-v3\n",
        "    q_values = np.zeros((n_points, n_points))  # Assuming two action dimensions\n",
        "\n",
        "    # Fix state dimensions (example: [0, 0, 0, 0, 0, 0, 0, 0] or custom values)\n",
        "    if fixed_state is None:\n",
        "        fixed_state = np.zeros(8)  # Default fixed state\n",
        "    elif len(fixed_state) != 8:\n",
        "        raise ValueError(\"fixed_state must have 8 dimensions for LunarLanderContinuous-v3\")\n",
        "\n",
        "    # Loop over possible action pairs\n",
        "    for i, a1 in enumerate(actions):\n",
        "        for j, a2 in enumerate(actions):\n",
        "            action = np.array([a1, a2])  # Two action dimensions\n",
        "            obs_tensor = torch.tensor(fixed_state.reshape(1, -1), dtype=torch.float32).to(model.device)\n",
        "            action_tensor = torch.tensor(action.reshape(1, -1), dtype=torch.float32).to(model.device)\n",
        "            with torch.no_grad():\n",
        "                # Evaluate Q-value for given state-action pair\n",
        "                q_value = model.critic_target(obs_tensor, action_tensor)[0].cpu().numpy()\n",
        "            q_values[i, j] = q_value\n",
        "\n",
        "    # Plot the Q-value heatmap\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.imshow(\n",
        "        q_values,\n",
        "        extent=[actions.min(), actions.max(), actions.min(), actions.max()],\n",
        "        origin='lower',\n",
        "        aspect='auto',\n",
        "        cmap='viridis',\n",
        "    )\n",
        "    plt.colorbar(label='Q-value')\n",
        "    plt.xlabel('Action Dimension 1')\n",
        "    plt.ylabel('Action Dimension 2')\n",
        "    plt.title('Q-value Heatmap for Fixed State')\n",
        "    plt.show()\n",
        "\n",
        "# Example usage:\n",
        "plot_q_values(model, fixed_state=np.zeros(8))  # Replace with a meaningful state if needed\n",
        "\n"
      ],
      "metadata": {
        "id": "2CjA5ZfeN3Wz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K2r80NW_e51O"
      },
      "source": [
        "# **Half-Cheetahv4 Implementation**"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "-7QKzaZbcomu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install optuna\n",
        "import os\n",
        "import math\n",
        "import optuna\n",
        "import gymnasium as gym\n",
        "from stable_baselines3 import SAC\n",
        "from stable_baselines3.common.monitor import Monitor\n",
        "from stable_baselines3.common.evaluation import evaluate_policy\n",
        "from stable_baselines3.common.callbacks import BaseCallback\n",
        "\n",
        "# ====== Define Cosine Annealing Schedule ======\n",
        "def cosine_annealing(initial_lr, min_lr):\n",
        "    def lr_schedule(progress_remaining):\n",
        "        return min_lr + (initial_lr - min_lr) * 0.5 * (1 + math.cos(math.pi * (1 - progress_remaining)))\n",
        "    return lr_schedule\n",
        "\n",
        "# ====== Environment Setup ======\n",
        "env_name = \"HalfCheetah-v4\"  # Change to HalfCheetah-v4\n",
        "log_dir = \"./monitor_logs/\"\n",
        "os.makedirs(log_dir, exist_ok=True)\n",
        "\n",
        "env = gym.make(env_name)\n",
        "env = Monitor(env, log_dir + \"monitor.csv\", allow_early_resets=True)\n",
        "eval_env = gym.make(env_name, render_mode=\"rgb_array\")\n",
        "eval_env = Monitor(eval_env)\n",
        "\n",
        "# ====== Callback to Track Training ======\n",
        "class TrainCallback(BaseCallback):\n",
        "    def __init__(self, verbose=0):\n",
        "        super(TrainCallback, self).__init__(verbose)\n",
        "        self.policy_losses = []\n",
        "        self.q_losses = []\n",
        "        self.entropy_coefficients = []\n",
        "\n",
        "    def _on_step(self) -> bool:\n",
        "        logs = self.logger.name_to_value\n",
        "        if \"train/actor_loss\" in logs:\n",
        "            self.policy_losses.append(logs[\"train/actor_loss\"])\n",
        "        if \"train/critic_loss\" in logs:\n",
        "            self.q_losses.append(logs[\"train/critic_loss\"])\n",
        "        if \"train/ent_coef\" in logs:\n",
        "            self.entropy_coefficients.append(logs[\"train/ent_coef\"])\n",
        "        return True\n",
        "\n",
        "# ====== Define Hyperparameter Tuning Function ======\n",
        "def optimize_sac(trial):\n",
        "    # Suggested hyperparameters\n",
        "    initial_lr = trial.suggest_float(\"initial_lr\", 1e-4, 1e-3, log=True)\n",
        "    min_lr = trial.suggest_float(\"min_lr\", 1e-5, 5e-4, log=True)\n",
        "    batch_size = trial.suggest_categorical(\"batch_size\", [256, 512, 1024])\n",
        "    target_entropy = trial.suggest_float(\"target_entropy\", -7.0, -5.0)  # Adjusted for higher-dimensional action space\n",
        "    gamma = trial.suggest_float(\"gamma\", 0.95, 0.99)\n",
        "    tau = trial.suggest_float(\"tau\", 0.005, 0.02)\n",
        "\n",
        "    # Cosine annealing learning rate schedule\n",
        "    learning_rate_schedule = cosine_annealing(initial_lr=initial_lr, min_lr=min_lr)\n",
        "\n",
        "    train_callback = TrainCallback()\n",
        "\n",
        "    # Initialize SAC model\n",
        "    model = SAC(\n",
        "        \"MlpPolicy\",\n",
        "        env,\n",
        "        learning_rate=learning_rate_schedule,\n",
        "        batch_size=batch_size,\n",
        "        ent_coef=\"auto\",\n",
        "        target_entropy=target_entropy,\n",
        "        gamma=gamma,\n",
        "        tau=tau,\n",
        "        verbose=0,\n",
        "        device=\"cuda\"  # Use GPU for faster training\n",
        "    )\n",
        "\n",
        "    # Train the model\n",
        "    model.learn(total_timesteps=100000, callback=train_callback)\n",
        "\n",
        "    # Evaluate the model\n",
        "    mean_reward, _ = evaluate_policy(model, eval_env, n_eval_episodes=5)  # Reduce eval episodes for speed\n",
        "    return mean_reward\n",
        "\n",
        "# ====== Run Optuna Hyperparameter Tuning ======\n",
        "study = optuna.create_study(direction=\"maximize\")\n",
        "study.optimize(optimize_sac, n_trials=10)\n",
        "\n",
        "# Print best hyperparameters\n",
        "print(\"Best Hyperparameters:\", study.best_params)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xu1HdMytwlEV",
        "outputId": "b2f6e7d0-cdaf-4fd4-a888-2329f6c40855"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: optuna in /usr/local/lib/python3.10/dist-packages (4.1.0)\n",
            "Requirement already satisfied: alembic>=1.5.0 in /usr/local/lib/python3.10/dist-packages (from optuna) (1.14.0)\n",
            "Requirement already satisfied: colorlog in /usr/local/lib/python3.10/dist-packages (from optuna) (6.9.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from optuna) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from optuna) (24.2)\n",
            "Requirement already satisfied: sqlalchemy>=1.4.2 in /usr/local/lib/python3.10/dist-packages (from optuna) (2.0.36)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from optuna) (4.66.6)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from optuna) (6.0.2)\n",
            "Requirement already satisfied: Mako in /usr/local/lib/python3.10/dist-packages (from alembic>=1.5.0->optuna) (1.3.8)\n",
            "Requirement already satisfied: typing-extensions>=4 in /usr/local/lib/python3.10/dist-packages (from alembic>=1.5.0->optuna) (4.12.2)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from sqlalchemy>=1.4.2->optuna) (3.1.1)\n",
            "Requirement already satisfied: MarkupSafe>=0.9.2 in /usr/local/lib/python3.10/dist-packages (from Mako->alembic>=1.5.0->optuna) (3.0.2)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/gymnasium/envs/registration.py:517: DeprecationWarning: \u001b[33mWARN: The environment HalfCheetah-v4 is out of date. You should consider upgrading to version `v5`.\u001b[0m\n",
            "  logger.deprecation(\n",
            "[I 2024-12-10 13:30:40,537] A new study created in memory with name: no-name-814595e3-76c1-45be-848b-d79282264107\n",
            "[I 2024-12-10 13:52:54,327] Trial 0 finished with value: 1330.2595348 and parameters: {'initial_lr': 0.00046413164744655417, 'min_lr': 0.00023018172093453582, 'batch_size': 512, 'target_entropy': -5.546692826262865, 'gamma': 0.9537028022136499, 'tau': 0.01707149573596157}. Best is trial 0 with value: 1330.2595348.\n",
            "[I 2024-12-10 14:15:47,520] Trial 1 finished with value: 1284.0552382 and parameters: {'initial_lr': 0.000179619573727657, 'min_lr': 0.0002788872960490493, 'batch_size': 1024, 'target_entropy': -5.841336981196641, 'gamma': 0.9851361261395278, 'tau': 0.018148196904756678}. Best is trial 0 with value: 1330.2595348.\n",
            "[I 2024-12-10 14:37:41,712] Trial 2 finished with value: 1052.4137364000003 and parameters: {'initial_lr': 0.0003870613504554683, 'min_lr': 0.00013526218374552352, 'batch_size': 512, 'target_entropy': -6.421627899560807, 'gamma': 0.9594012558036106, 'tau': 0.018978208986881455}. Best is trial 0 with value: 1330.2595348.\n",
            "[I 2024-12-10 14:59:26,496] Trial 3 finished with value: 1298.5991384000001 and parameters: {'initial_lr': 0.0007727279846613634, 'min_lr': 5.969202467295858e-05, 'batch_size': 512, 'target_entropy': -6.863996960906284, 'gamma': 0.9515875191211633, 'tau': 0.0059966876456556675}. Best is trial 0 with value: 1330.2595348.\n",
            "[I 2024-12-10 15:21:00,989] Trial 4 finished with value: 4162.855603 and parameters: {'initial_lr': 0.00020064737336115973, 'min_lr': 0.000339904038366785, 'batch_size': 256, 'target_entropy': -5.7775972692026825, 'gamma': 0.9778443046757581, 'tau': 0.014802008753338598}. Best is trial 4 with value: 4162.855603.\n",
            "[I 2024-12-10 15:43:20,616] Trial 5 finished with value: 3404.8324754 and parameters: {'initial_lr': 0.0001353746684124462, 'min_lr': 0.0002655929807489227, 'batch_size': 1024, 'target_entropy': -6.335192407713829, 'gamma': 0.9712334230210512, 'tau': 0.016097563559140472}. Best is trial 4 with value: 4162.855603.\n",
            "[I 2024-12-10 16:05:43,476] Trial 6 finished with value: 1315.8237998 and parameters: {'initial_lr': 0.00022625113370165986, 'min_lr': 0.00015534825426358537, 'batch_size': 1024, 'target_entropy': -6.833869199834906, 'gamma': 0.9558535606810865, 'tau': 0.019749066108812607}. Best is trial 4 with value: 4162.855603.\n",
            "[I 2024-12-10 16:28:14,864] Trial 7 finished with value: 3418.8764484 and parameters: {'initial_lr': 0.00038220434655394464, 'min_lr': 4.4478292548852246e-05, 'batch_size': 1024, 'target_entropy': -5.677202501530998, 'gamma': 0.9794283763403162, 'tau': 0.0064716813248967674}. Best is trial 4 with value: 4162.855603.\n",
            "[I 2024-12-10 16:50:08,501] Trial 8 finished with value: 1231.3207956 and parameters: {'initial_lr': 0.00040127896789728476, 'min_lr': 0.00018235799365638538, 'batch_size': 512, 'target_entropy': -6.539188071627103, 'gamma': 0.960908937385383, 'tau': 0.015864903942596775}. Best is trial 4 with value: 4162.855603.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install stable-baselines3[extra]\n",
        "!pip install gymnasium\n",
        "!pip install matplotlib\n",
        "!pip install gymnasium[mujoco]\n",
        "import json\n",
        "import os\n",
        "import math\n",
        "import gymnasium as gym\n",
        "from stable_baselines3 import SAC\n",
        "from stable_baselines3.common.monitor import Monitor\n",
        "from stable_baselines3.common.evaluation import evaluate_policy\n",
        "from stable_baselines3.common.callbacks import EvalCallback, BaseCallback\n",
        "\n",
        "# Best parameters from your trial\n",
        "best_params = {\n",
        "    \"initial_lr\": 0.00020064737336115973,\n",
        "    \"min_lr\": 0.000339904038366785,\n",
        "    \"batch_size\": 256,\n",
        "    \"target_entropy\": -5.7775972692026825,\n",
        "    \"gamma\": 0.9778443046757581,\n",
        "    \"tau\": 0.014802008753338598\n",
        "}\n",
        "\n",
        "# Save the parameters to a JSON file\n",
        "with open(\"best_sac_params.json\", \"w\") as f:\n",
        "    json.dump(best_params, f)\n",
        "\n",
        "print(\"Best parameters saved to best_sac_params.json\")\n",
        "\n",
        "# ====== Define Cosine Annealing Schedule ======\n",
        "def cosine_annealing(initial_lr, min_lr):\n",
        "    def lr_schedule(progress_remaining):\n",
        "        return min_lr + (initial_lr - min_lr) * 0.5 * (1 + math.cos(math.pi * (1 - progress_remaining)))\n",
        "    return lr_schedule\n",
        "\n",
        "# ====== Environment Setup ======\n",
        "env_name = \"HalfCheetah-v4\"  # Change to HalfCheetah-v4\n",
        "log_dir = \"./monitor_logs/\"\n",
        "os.makedirs(log_dir, exist_ok=True)\n",
        "\n",
        "env = gym.make(env_name)\n",
        "env = Monitor(env, log_dir + \"monitor.csv\", allow_early_resets=True)\n",
        "eval_env = gym.make(env_name, render_mode=\"rgb_array\")\n",
        "eval_env = Monitor(eval_env)\n",
        "\n",
        "# ====== Callback to Track Training ======\n",
        "class TrainCallback(BaseCallback):\n",
        "    def __init__(self, verbose=0):\n",
        "        super(TrainCallback, self).__init__(verbose)\n",
        "        self.policy_losses = []\n",
        "        self.q_losses = []\n",
        "        self.entropy_coefficients = []\n",
        "\n",
        "    def _on_step(self) -> bool:\n",
        "        logs = self.logger.name_to_value\n",
        "        if \"train/actor_loss\" in logs:\n",
        "            self.policy_losses.append(logs[\"train/actor_loss\"])\n",
        "        if \"train/critic_loss\" in logs:\n",
        "            self.q_losses.append(logs[\"train/critic_loss\"])\n",
        "        if \"train/ent_coef\" in logs:\n",
        "            self.entropy_coefficients.append(logs[\"train/ent_coef\"])\n",
        "        return True\n",
        "\n",
        "    def save_logs(self, filename):\n",
        "        data = {\n",
        "            \"policy_losses\": self.policy_losses,\n",
        "            \"q_losses\": self.q_losses,\n",
        "            \"entropy_coefficients\": self.entropy_coefficients\n",
        "        }\n",
        "        with open(filename, \"w\") as f:\n",
        "            json.dump(data, f)\n",
        "\n",
        "# Load parameters\n",
        "with open(\"best_sac_params.json\", \"r\") as f:\n",
        "    best_params = json.load(f)\n",
        "\n",
        "# Set up learning rate schedule\n",
        "learning_rate_schedule = cosine_annealing(\n",
        "    initial_lr=best_params[\"initial_lr\"],\n",
        "    min_lr=best_params[\"min_lr\"]\n",
        ")\n",
        "\n",
        "# Initialize SAC with the best parameters\n",
        "model = SAC(\n",
        "    \"MlpPolicy\",\n",
        "    env,\n",
        "    learning_rate=learning_rate_schedule,\n",
        "    batch_size=best_params[\"batch_size\"],\n",
        "    ent_coef=\"auto\",  # Enable automatic entropy tuning\n",
        "    target_entropy=best_params[\"target_entropy\"],\n",
        "    gamma=best_params[\"gamma\"],\n",
        "    tau=best_params[\"tau\"],\n",
        "    verbose=1,\n",
        "    tensorboard_log=\"./sac_tensorboard/\",\n",
        "    device=\"cuda\"  # Use GPU if available\n",
        ")\n",
        "\n",
        "eval_callback = EvalCallback(eval_env, best_model_save_path=\"./logs/\",\n",
        "                             log_path=\"./logs/\", eval_freq=1000)\n",
        "\n",
        "train_callback = TrainCallback()\n",
        "\n",
        "# Train the model\n",
        "model.learn(total_timesteps=1000, callback=[eval_callback, train_callback])\n",
        "\n",
        "train_callback.save_logs(\"training_logs.json\")\n",
        "print(\"Training statistics saved to training_logs.json\")\n",
        "\n",
        "# Save the trained model\n",
        "model.save(\"sac_halfcheetah_100_model\")\n",
        "print(\"Trained model saved as sac_halfcheetah_100_model.zip\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JNh1PNf94aN4",
        "outputId": "8f919afa-9cfd-450f-b113-341fd144497a"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: stable-baselines3[extra] in /usr/local/lib/python3.10/dist-packages (2.4.0)\n",
            "Requirement already satisfied: gymnasium<1.1.0,>=0.29.1 in /usr/local/lib/python3.10/dist-packages (from stable-baselines3[extra]) (1.0.0)\n",
            "Requirement already satisfied: numpy<2.0,>=1.20 in /usr/local/lib/python3.10/dist-packages (from stable-baselines3[extra]) (1.26.4)\n",
            "Requirement already satisfied: torch>=1.13 in /usr/local/lib/python3.10/dist-packages (from stable-baselines3[extra]) (2.5.1+cu121)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.10/dist-packages (from stable-baselines3[extra]) (3.1.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from stable-baselines3[extra]) (2.2.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from stable-baselines3[extra]) (3.8.0)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.10/dist-packages (from stable-baselines3[extra]) (4.10.0.84)\n",
            "Requirement already satisfied: pygame in /usr/local/lib/python3.10/dist-packages (from stable-baselines3[extra]) (2.6.1)\n",
            "Requirement already satisfied: tensorboard>=2.9.1 in /usr/local/lib/python3.10/dist-packages (from stable-baselines3[extra]) (2.17.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from stable-baselines3[extra]) (5.9.5)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from stable-baselines3[extra]) (4.66.6)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from stable-baselines3[extra]) (13.9.4)\n",
            "Requirement already satisfied: ale-py>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from stable-baselines3[extra]) (0.10.1)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.10/dist-packages (from stable-baselines3[extra]) (11.0.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from ale-py>=0.9.0->stable-baselines3[extra]) (4.12.2)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from gymnasium<1.1.0,>=0.29.1->stable-baselines3[extra]) (0.0.4)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (1.4.0)\n",
            "Requirement already satisfied: grpcio>=1.48.2 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (1.68.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (3.7)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (24.2)\n",
            "Requirement already satisfied: protobuf!=4.24.0,>=3.19.6 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (4.25.5)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (75.1.0)\n",
            "Requirement already satisfied: six>1.9 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (1.16.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (3.1.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable-baselines3[extra]) (3.16.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable-baselines3[extra]) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable-baselines3[extra]) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable-baselines3[extra]) (2024.10.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable-baselines3[extra]) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=1.13->stable-baselines3[extra]) (1.3.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable-baselines3[extra]) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable-baselines3[extra]) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable-baselines3[extra]) (4.55.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable-baselines3[extra]) (1.4.7)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable-baselines3[extra]) (3.2.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable-baselines3[extra]) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->stable-baselines3[extra]) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->stable-baselines3[extra]) (2024.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->stable-baselines3[extra]) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->stable-baselines3[extra]) (2.18.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->stable-baselines3[extra]) (0.1.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard>=2.9.1->stable-baselines3[extra]) (3.0.2)\n",
            "Requirement already satisfied: gymnasium in /usr/local/lib/python3.10/dist-packages (1.0.0)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (1.26.4)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (3.1.0)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (4.12.2)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (0.0.4)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (3.8.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (4.55.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.4.7)\n",
            "Requirement already satisfied: numpy<2,>=1.21 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (24.2)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (11.0.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (3.2.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
            "Requirement already satisfied: gymnasium[mujoco] in /usr/local/lib/python3.10/dist-packages (1.0.0)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium[mujoco]) (1.26.4)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium[mujoco]) (3.1.0)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium[mujoco]) (4.12.2)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from gymnasium[mujoco]) (0.0.4)\n",
            "Requirement already satisfied: mujoco>=2.1.5 in /usr/local/lib/python3.10/dist-packages (from gymnasium[mujoco]) (3.2.6)\n",
            "Requirement already satisfied: imageio>=2.14.1 in /usr/local/lib/python3.10/dist-packages (from gymnasium[mujoco]) (2.36.1)\n",
            "Requirement already satisfied: pillow>=8.3.2 in /usr/local/lib/python3.10/dist-packages (from imageio>=2.14.1->gymnasium[mujoco]) (11.0.0)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from mujoco>=2.1.5->gymnasium[mujoco]) (1.4.0)\n",
            "Requirement already satisfied: etils[epath] in /usr/local/lib/python3.10/dist-packages (from mujoco>=2.1.5->gymnasium[mujoco]) (1.11.0)\n",
            "Requirement already satisfied: glfw in /usr/local/lib/python3.10/dist-packages (from mujoco>=2.1.5->gymnasium[mujoco]) (2.8.0)\n",
            "Requirement already satisfied: pyopengl in /usr/local/lib/python3.10/dist-packages (from mujoco>=2.1.5->gymnasium[mujoco]) (3.1.7)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from etils[epath]->mujoco>=2.1.5->gymnasium[mujoco]) (2024.10.0)\n",
            "Requirement already satisfied: importlib_resources in /usr/local/lib/python3.10/dist-packages (from etils[epath]->mujoco>=2.1.5->gymnasium[mujoco]) (6.4.5)\n",
            "Requirement already satisfied: zipp in /usr/local/lib/python3.10/dist-packages (from etils[epath]->mujoco>=2.1.5->gymnasium[mujoco]) (3.21.0)\n",
            "Best parameters saved to best_sac_params.json\n",
            "Using cuda device\n",
            "Wrapping the env in a DummyVecEnv.\n",
            "Logging to ./sac_tensorboard/SAC_7\n",
            "Eval num_timesteps=1000, episode_reward=-4.53 +/- 1.08\n",
            "Episode length: 1000.00 +/- 0.00\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 1e+03    |\n",
            "|    mean_reward     | -4.53    |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 1000     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -39.1    |\n",
            "|    critic_loss     | 1.82     |\n",
            "|    ent_coef        | 0.781    |\n",
            "|    ent_coef_loss   | -2.33    |\n",
            "|    learning_rate   | 0.00034  |\n",
            "|    n_updates       | 899      |\n",
            "---------------------------------\n",
            "New best mean reward!\n",
            "Training statistics saved to training_logs.json\n",
            "Trained model saved as sac_halfcheetah_100_model.zip\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "from google.colab import files\n",
        "\n",
        "# Compress monitor logs\n",
        "shutil.make_archive(\"monitor_logs\", 'zip', \"./monitor_logs/\")\n",
        "files.download(\"monitor_logs.zip\")\n",
        "\n",
        "# Compress TensorBoard logs\n",
        "shutil.make_archive(\"sac_tensorboard\", 'zip', \"./sac_tensorboard/\")\n",
        "files.download(\"sac_tensorboard.zip\")\n",
        "\n",
        "# Download model\n",
        "files.download(\"sac_halfcheetah_500k_model.zip\")\n",
        "\n",
        "# Download training logs\n",
        "files.download(\"training_logs.json\")\n",
        "\n",
        "# Download evaluation logs\n",
        "files.download(\"./logs/evaluations.npz\")  # Update path if necessary\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "tx7tATlP1aqF",
        "outputId": "decc0fd8-94e8-4d65-a288-f205af0fe560"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_a1a07a62-2eb5-476e-8bac-e34ab4823f5c\", \"monitor_logs.zip\", 6774)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_2dafc5f2-906d-468c-af19-c9aacd213a38\", \"sac_tensorboard.zip\", 77072)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_d2aa4870-4af1-4065-8b4c-e8fee0311c73\", \"sac_halfcheetah_500k_model.zip\", 3241930)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_156247cb-44ef-4a39-a13b-81528ccc3a08\", \"training_logs.json\", 30422241)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_93f4046f-842b-4626-9bdc-6e7e3ccd6bda\", \"evaluations.npz\", 44770)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.download(\"sac_halfcheetah_100_model.zip\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "iywkktBo6qS-",
        "outputId": "ca0a8804-5b87-4f6f-f936-4b72e6c5af0f"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_b32f211f-f70d-42cc-8e5f-aea0ae3b3287\", \"sac_halfcheetah_100_model.zip\", 3237825)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ====== 6. Plot Learning Curve ======\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import math\n",
        "import torch\n",
        "\n",
        "model_path = \"sac_halfcheetah_optimal_model.zip\"\n",
        "model = SAC.load(model_path)\n",
        "\n",
        "monitor_file = log_dir + \"monitor.csv\"\n",
        "if os.path.exists(monitor_file):\n",
        "    log_data = pd.read_csv(monitor_file, skiprows=1)\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.plot(log_data['l'].cumsum(), log_data['r'], label='Reward per Episode')\n",
        "    plt.xlabel('Timesteps')\n",
        "    plt.ylabel('Reward')\n",
        "    plt.title('Learning Curve (SAC on HalfCheetah-v4)')\n",
        "    plt.legend()\n",
        "    plt.grid()\n",
        "    plt.show()\n",
        "else:\n",
        "    print(\"Monitor log file not found. Ensure the environment is wrapped correctly.\")\n",
        "\n",
        "# ====== 7. Evaluate the Agent ======\n",
        "mean_reward, std_reward = evaluate_policy(model, eval_env, n_eval_episodes=10)\n",
        "print(f\"Mean reward: {mean_reward:.2f} ± {std_reward:.2f}\")\n",
        "\n",
        "# ====== 8. Visualize Policy Action Distribution ======\n",
        "def visualize_action_distribution_all_dims(model, env, n_samples=1000):\n",
        "    \"\"\"\n",
        "    Visualizes the distribution of actions for all action dimensions.\n",
        "    \"\"\"\n",
        "    actions = []\n",
        "    obs, _ = env.reset()\n",
        "    for _ in range(n_samples):\n",
        "        action, _ = model.predict(obs, deterministic=False)  # Sample stochastic actions\n",
        "        actions.append(action)\n",
        "        obs, _, terminated, truncated, _ = env.step(action)\n",
        "        if terminated or truncated:\n",
        "            obs, _ = env.reset()\n",
        "\n",
        "    actions = np.array(actions)\n",
        "    n_dims = actions.shape[1]\n",
        "\n",
        "    # Create subplots for each action dimension\n",
        "    fig, axes = plt.subplots(1, n_dims, figsize=(15, 5), sharey=True)\n",
        "    for i in range(n_dims):\n",
        "        axes[i].hist(actions[:, i], bins=50, density=True, alpha=0.7, label=f'Dim {i+1}')\n",
        "        axes[i].set_title(f'Action Dim {i+1}')\n",
        "        axes[i].set_xlabel('Action Value')\n",
        "        axes[i].grid(True)\n",
        "\n",
        "    plt.suptitle('Policy Action Distribution for All Dimensions')\n",
        "    plt.ylabel('Density')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Call the function\n",
        "visualize_action_distribution_all_dims(model, env)\n",
        "\n",
        "# ====== 9. Visualize Q-Value Heatmaps ======\n",
        "def plot_q_values_all_pairs(model, env, n_points=50):\n",
        "    \"\"\"\n",
        "    Plots Q-value heatmaps for all pairs of action dimensions.\n",
        "    Fixes other dimensions and varies two dimensions at a time.\n",
        "    \"\"\"\n",
        "    obs, _ = env.reset()  # Get a representative observation\n",
        "    obs_tensor = torch.tensor(obs.reshape(1, -1), dtype=torch.float32).to(model.device)\n",
        "\n",
        "    n_dims = env.action_space.shape[0]\n",
        "    action_ranges = np.linspace(-1, 1, n_points)  # Adjust range if needed\n",
        "    fig, axes = plt.subplots(n_dims, n_dims, figsize=(6, 6))\n",
        "\n",
        "    for i in range(n_dims):\n",
        "        for j in range(n_dims):\n",
        "            if i == j:\n",
        "                # Plot diagonal as empty or fixed\n",
        "                axes[i, j].axis('off')\n",
        "                continue\n",
        "\n",
        "            # Create grid for two action dimensions\n",
        "            action_grid = np.meshgrid(action_ranges, action_ranges)\n",
        "            q_values = np.zeros((n_points, n_points))\n",
        "\n",
        "            for x in range(n_points):\n",
        "                for y in range(n_points):\n",
        "                    action = np.zeros(n_dims)\n",
        "                    action[i] = action_grid[0][x, y]\n",
        "                    action[j] = action_grid[1][x, y]\n",
        "\n",
        "                    # Convert action to tensor\n",
        "                    action_tensor = torch.tensor(action.reshape(1, -1), dtype=torch.float32).to(model.device)\n",
        "                    with torch.no_grad():\n",
        "                        q_value = model.critic_target(obs_tensor, action_tensor)[0].cpu().numpy()\n",
        "                    q_values[x, y] = q_value\n",
        "\n",
        "            # Plot heatmap for the pair (i, j)\n",
        "            im = axes[i, j].imshow(q_values, extent=[-1, 1, -1, 1], origin='lower', aspect='auto', cmap='viridis')\n",
        "            axes[i, j].set_title(f'Dim {i+1} vs Dim {j+1}')\n",
        "            axes[i, j].set_xlabel(f'Dim {i+1}')\n",
        "            axes[i, j].set_ylabel(f'Dim {j+1}')\n",
        "\n",
        "    fig.suptitle('Q-Value Heatmaps for Action Dimension Pairs', fontsize=16)\n",
        "    fig.colorbar(im, ax=axes, orientation='horizontal', fraction=0.03, pad=0.07)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Call the function\n",
        "plot_q_values_all_pairs(model, env)\n",
        "\n",
        "# ====== Optional: TensorBoard ======\n",
        "%load_ext tensorboard\n",
        "%tensorboard --logdir ./sac_tensorboard/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 207
        },
        "id": "0YON3kZf46kL",
        "outputId": "b961670a-9e17-47b7-da57-acce98f04520"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'log_dir' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-8a47a273279b>\u001b[0m in \u001b[0;36m<cell line: 9>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mmonitor_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlog_dir\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"monitor.csv\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmonitor_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mlog_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmonitor_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mskiprows\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'log_dir' is not defined"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}